# Default values for gen3kubernetes.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
#
# Pay strict attention to the database_servername and the various database passwords

replicaCount: 1
config_helper: "assets/config_helper.py"

#global tag for lables, etc
#Thi is what will be used to name all your services and your url, pick a name that works for you.
ENV: gen3
database_servername: "postgres-k8sgen3.postgres.database.azure.com" #this needs to be changed.
database_port: "5432"
#dictionary_url: "https://s3.amazonaws.com/dictionary-artifacts/datadictionary/develop/schema.json"
path_to_schema_dir: "/schemas_dir/schemas"
kubernetescluster:
  # This section can be used to define your cluster admins and developers.
  # uncomment the admins and contributors sections and add your users.
  # users are identified by their servicePrincipalIdentifier
  # which is usually their username.  Groups are identified by the OID of the group.
  # Skip this unless you really know what you are doing in both Azure IAM and Kubernetes.
  administrators:
  #   - "user1@yourcompany.com"
  #   - "user2@yourcompany.com"
  administratorgroupoids:
  #   - "123-4567-99873-223"
  contributors:
  #   - developer1@yourcompany.com
  #   - developer2@yourcompany.com
  contributorgroupsoid:
  #   - "987-5433-24865-099"

fence:
  serviceport: 80
  database:
    username: "fence_gen3dev_user"
    databasename: "fence_db"
    db_password: "b65a802fdf7a101c619e35354e791eeb"
  defaultIDPProvider: google
  enabledIDPProviders:
    - name: "google"
      loginButtonText: "Google Login"
    # - name: "microsoft"
    #   loginButtonText: "Microsoft Login"
    # - name: "okta"
    #   loginButtonText: "Okta Login"
  replicas: 1
#  image: "quay.io/cdis/fence:2021.03"
#  image: "rmolinger/fence:katie02"
  image: "quay.io/cdis/fence:2021.03"
  base_url: 'https://mysitename.mydomain.com/user'
  resources:
    limits:
      cpu: '500m'
      memory: '512Mi'
    requests:
      cpu: '500m'
      memory: '256Mi'
  googleOauth:
    client_id: 'xxx.apps.googleusercontent.com'
    client_secret: 'yyyy-tH7O7'
  microsoftOauth:
    client_id: 'mmmmmmmm-nnn-40oooo93-a9e8'
    client_secret: 'm-99-uibgyugvbijknlhj-_D.I'
  oktaOauth:
    client_id: ""
  defaultLoginURLSuffix: 'login/google'
  amazonStorageCreds:
    - name: 'CREDS1'
      accesskeyid: 'GADFGDFSHSTRDFAGADF'
      secretkey: '+jiogdfanklmnlkmnlmadfgjlojlk;fgdsmnlk+nzloas6rSp0GsZuOK+Q+oT7gi5Cpdj8TEvqaC5xvOQ=='
    - name: 'CREDS2'
      accesskeyid: 'HGERTTRGFDSGFDSDFGBDFA'
      secretkey: '2+iongfkoafd9987nkljlgfdankl/njfgdsanbji'
  amazonBuckets:
    - name: 'bucket1'
      credentialname: 'CREDS1'
      endpoint_url: 'https://minio-app-service-bfncrxya.azurewebsites.net'
      region: 'us-east-1'
    - name: 'bucket2'
      credentialname: 'CREDS2'
      region: 'us-east-1'
  azCredentials:
    - name: 'CREDA'
      az_stgacctname: 'stgacctaaaa'
      az_connection_string: 'DefaultEndpointsProtocol=https;AccountName=stgacctaaaa;AccountKey=nkljKLJnLKNklNLKn98y789bJG/klnklnl==;EndpointSuffix=core.windows.net'
  azureBlobstores:
    - name: 'azgen3blobstorage'
      credentialname: 'CREDA'
  dataUploadBucket: 'azgen3blobstorage'
  adminUsers:
    - "sidney@gmail.com"
    - "alex@gmail.com"
    - "rodney@gmail.com"
    - "mary@gmail.com"
    - "Alam@gmail.com"
    - "kick.me@gmail.com"
  regularUsers:
    - "foo@mydomain.com"
    - "bar@mydomain.com"
  service:
    type: ClusterIP
    httpPort: 80
    httpsPort: 443
  cacrtFiles:
    - name: "myprivatecert.crt"
      fileLocation: "assets/myprivatecert.crt"
  #While not necessary, you should consider setting up your own JWT keys for your instance.
  jwt_private_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEwAIBADANBgkqhkiG9w0BAQEFAASCBKowggSmAgEAAoIBAQDkS+nTVWUnXgIQ
    11LICu8p5XOOKk4XliZWcOef8ik5hC2nydh6BgUeMsP/8hUb9q+MpKrpnI0Q1itR
    rI17HNlF47uN7oMsMBEXUbOQt7dT6ACoKhXUKHHegXX6ILVVSPxJgzgO4U3LkK03
    DF8V0XtE0O7ireOaecyObl1S4I61gxyWGllyCnvQoRVHF07ZbPK46xQD9Hs9lyIq
    SVq/Y4K4qchY26g4ZQ78lpPCCF3sd4wwI9DIGOTigKLgI+G6MxUSRo3z8C452Djv
    4BFHBdrbu6fTmxn5WumdN1cjksA7cyUYCEQ/zr3bVlIPEi0C3TlM6J485VDl/e+z
    NZDxE+CrAgMBAAECggEBALjXFfy72WikXrvP+wBoife8BXGHoy7JrUSzqXYBFYuy
    D10YuiMiGKa4p6TlFl5Lvxr6gD8hN734CZ7ZPLv/QZOqrUJfaOkwCcZ9qgt0FATl
    hRKi8IWUMY3b9++K6AajT1Ed4xd5KAm3m+dEJwB9WXcAS0pRTETYdNylUv41PpaK
    h+MwyBBF5NSGnRYQuLpSqTD8jRfaXOzsfUXQmElgQ0ywTVEQPyoWHaB2CQzWdRbv
    ChwPBOfJHh4wsEO2C+sLTcQ0/DL4wC6yahF6iqnfBNR0bmcsl1696RkZ+ricEPY8
    8pbefY9Kp4GvFd2bFjJfArCY8OhqtqeniKjUeOkOJnECgYEA9h1bG+g6G2SJZFhN
    1fEpT5tO4r0RA4ZBnlPVIaxOcBLgJGMsTsN/Olse1so4Mm0gFiT17icoRRde5XKj
    DPp6E2PSyafQgLF48OpEFpDNJRcSUx+SPchO5XbXS0E1g41SSV1iUGX5l8z5ooHZ
    oRh3nDR5FpqFmrskGZGGzYSVVgcCgYEA7XdYPW1W92s3l8ixfZAQmIhFvHt9BP3t
    +rQQth4DF+RiGw+9VurVGbcmnuDBFD79whU1dv07vimaJhRwzkJsYRWY7GipQjfb
    eJiP3noAykji8QLBuXLMn1nC0zPgPDI+fw9kGG6BN47TwffqegnY3e34x+Z6niOu
    RPChgbwuVz0CgYEAn+7bsfrrJ7nhfo/TpN+elNi8HqP2Vm/8XELmEYfDQRlDBuJb
    fKOWHfnwxGuMsVgwrbM/DY/sn1lclYY1Zs2uXsNTyy9+UuMBUK0rqe5XIW8ovVOa
    2QM+IedkaMbrW52Oh5ZjuRNLatYMswnmvsUDlUKUHZg0kiRihSsIiq1JzikCgYEA
    pJtm8mW5tBaBWdjHmCdmKaXE2WusdOpkrbVmJCfXsxcgHmwhrn9oKsQ3pEhd7t/f
    XUsAXbQaVq5V6XGf5IpMwFuNPssqEPnRS95NVEW0CITPs4taqGd4ijlZgLfkBzY4
    9jDADOOE9PMvvLRSuDfdObG9Eyyj8L0BD07SVbERWm0CgYEA36QYn6P84onE9lQ8
    JpXhzhf/fdywKF1aXABhK5qhoGSHUxLiQ9G2VIXO4JYshZj0O0wa7nJctKgl0Hs5
    fv6xmI5lSSi/Mzcw5FwB+zd9XwjuCx7/TvE5Q4yzj30ihC2gmtDEE/X0Wcy+o2Dc
    D8OEKlWXGVIdG6KbKLcdbNoOFjo=
    -----END PRIVATE KEY-----
  jwt_public_key: |
    -----BEGIN PUBLIC KEY-----
    MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5Evp01VlJ14CENdSyArv
    KeVzjipOF5YmVnDnn/IpOYQtp8nYegYFHjLD//IVG/avjKSq6ZyNENYrUayNexzZ
    ReO7je6DLDARF1GzkLe3U+gAqCoV1Chx3oF1+iC1VUj8SYM4DuFNy5CtNwxfFdF7
    RNDu4q3jmnnMjm5dUuCOtYMclhpZcgp70KEVRxdO2WzyuOsUA/R7PZciKklav2OC
    uKnIWNuoOGUO/JaTwghd7HeMMCPQyBjk4oCi4CPhujMVEkaN8/AuOdg47+ARRwXa
    27un05sZ+VrpnTdXI5LAO3MlGAhEP86921ZSDxItAt05TOiePOVQ5f3vszWQ8RPg
    qwIDAQAB
    -----END PUBLIC KEY-----
arborist:
  database:
    username: "arborist_gen3dev_user"
    databasename: "arborist_db"
    db_password: "E98CF2-8DC7HGES"
  replicas: 1
  serviceport: 80
  image: "quay.io/cdis/arborist:2021.03"
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 200m
      memory: 128Mi
  service:
    type: ClusterIP
    port: 80

image:
  pullPolicy: IfNotPresent
  #pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets:
  - name: "registrypullsecret"
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "gen3"

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: azure/application-gateway
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    #nginx.ingress.kubernetes.io/rewrite-target: /$1
    appgw.ingress.kubernetes.io/appgw-ssl-certificate: g3vnet-sslcert
    appgw.ingress.kubernetes.io/backend-protocol: "http"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"  hosts:
    - host: mysitename.mydomain.com
      paths:
        - path :
          actualPath: "/"
          serviceName: service-revproxy-<$ENV>
          servicePort: 80
        - path:
          actualPath: "/dropboxdev"
          serviceName: service-minio-gen3k8dev
          servicePort: 9000

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

# tell revproxy where to find the certificates, relative to the template.
revproxy:
  replicas: 2
  serviceport: 80
  image: "quay.io/cdis/nginx:1.17.6-ctds-1.0.1"
  resources:
    limits:
      cpu: '100m'
      memory: '256Mi'
    requests:
      cpu: '50m'
      memory: '128Mi'

peregrine:
  image: "quay.io/cdis/peregrine:2021.03"
  database:
    username: "peregrine_gen3dev_user"
    databasename: "metadata_db"
    db_password: "8ACB-0053084C2A97"
  serviceport: 80
  gdcapi_secret_key: "1JMWnHdApSGMJ8OIqA0IwWUEo8nJ1NJqwDQbjrz5L5v1QtW2ke"
  hmac_key: "1JMWnHdApSGMJ8OIqA0IwWUEo8nJ1NJqwDQbjrz5L5v1QtW2ke"
  bagitbucket: "noIdeaWhatThisIs"
  s3access: "notSureThisIsUsed"
  s3secret: "notSureThisIsUsed"
  schemas: assets/compose-services-schema.tgz
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi
tube:
  image: "quay.io/cdis/tube:2021.03"
  serviceport: 80
  replicas: 1
  etlmappinglocation: "assets/etlMapping.yaml"
  startupfileLocation: "assets/tube_startup.ksh"
  esrootcalocation: "assets/es-root-ca.pem"
  extrarunetllocation: "assets/run_optum_etl.py"
  etlmappinglocation: assets/compose-services-etlmapping.yaml
  elasticusername: "gen3"
  elasticpasswordb64: ""
  resources:
    limits:
      cpu: 500m
      memory: 2200M
    requests:
      cpu: 200m
      memory: 256Mi
  elastic:
    url: "extra-client-service.gen3elastic.svc.cluster.local"
    user: admin
    pass: admin

sheepdog:
  database:
    username: "sheepdog_gen3dev_user"
    databasename: "metadata_db"
    db_password: "E24B9254-7019-4081"
  replicas: 1
  serviceport: 80
  image: "quay.io/cdis/sheepdog:2021.03"
  s3access: "notSureThisIsUsed"
  s3secret: "notSureThisIsUsed"
  schemas: assets/compose-services-schema.tgz
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi
  service:
    type: ClusterIP
    port: 80
indexd:
  database:
    username: "indexd_gen3dev_user"
    databasename: "indexd_db"
    db_password: "D7E-2A715381173D"
  username: "indexd_client_gen3v"
  password:  "indexd_client_pass_gen3v"
  local_settings: "assets/indexd_settings.py"
  replicas: 1
  serviceport: 80
  image: "quay.io/cdis/indexd:2021.03"
  resources:
    limits:
      cpu: 1
      memory: "2000Mi"
    requests:
      cpu: "500m"
      memory: "1000Mi"
  service:
    type: "ClusterIP"
    port: 80
portal:
  replicas: 1
  serviceport: 80
  image: "quay.io/cdis/data-portal:2021.03"
  externalhostname: "mysitename.mydomain.com"
  gitops: "assets/gitops.png"
  gitopslogo: "assets/gitopslogo.png"
  workspaceurlsuffix: /lw-workspace/proxy/tree
  initialDelaySeconds: 300
  resources:
    limits:
      cpu: '2000m'
      memory: '4096Mi'
    requests:
      cpu: '2000m'
      memory: '2048Mi'
pidgin:
  image: "quay.io/cdis/pidgin:2021.03"
  replicas: 1
  serviceport: 80
  resources:
    limits:
      cpu: 250m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
guppy:
  image: "quay.io/cdis/guppy:2021.03"
  replicas: 1
  serviceport: 80
  resources:
    limits:
      cpu: 250m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
jupyter:
  image: "quay.io/occ_data/jupyternotebook:1.7.2"
  replicas: 1
  serviceport: 8888
  userDirSize: "32Gi"
  resources:
    limits:
      cpu: 2000m
      memory: 4000Mi
    requests:
      cpu: 2000m
      memory: 3000Mi
spark:
  image: "quay.io/cdis/gen3-spark:2020.05"
  replicas: 1
  pullPolicy: IfNotPresent
  spark_master: "http://<service-spark-ENV>"
  userDirSize: 32Gi
  spark_executor_memory: 2G
  spark_driver_memroy: 2G
  serviceports:
    - port: 9000
      name: "ninethousand"
    - port: 8030
      name: "eightythirty"
    - port: 8031
      name: "eightythirtyone"
    - port: 8032
      name: "eightythirtytwo"
    - port: 22
      name: ssh
  environment:
    - name:  "HADOOP_URL"
      value: "hdfs://0.0.0.0:9000"
    - name: "HADOOP_HOST"
      value: "0.0.0.0"

kibana:
  image: docker.elastic.co/kibana/kibana-oss:6.5.4
  serviceport: 5601
  resources:
    limits:
      cpu: 2000m
      memory: 4096Mi
    requests:
      cpu: 500m
      memory: 1024Mi
#
# The block below is optional as the minio dropbox S3 emulator is not yet working.
minio:
  image: "minio/minio"
  pullPolicy: IfNotPresent
  serviceport: 9000
  AZURE_STORAGE_ACCOUNT: stgacctdropaaaa
  AZURE_STORAGE_KEY: "noadfnIJOIONOonoer9089054j+8998jignjogsdfoieonko34ojn34no=="
  MINIO_ROOT_USER: "dropgen3dev"
  MINIO_ROOT_PASSWORD: "NUMxODUxQjUtREZGRC00NEE1LTlFRDItRTVCREI2MjFDNDY0Cg=="
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 200m
      memory: 512Mi
