---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-{{ .Values.ENV }}
  labels:
    {{- include "gen3kubernetes.labels" . | nindent 4 }}
    component: "spark"
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.spark.replicas }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "gen3kubernetes.selectorLabels" . | nindent 6 }}
      component: "spark"
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "gen3kubernetes.selectorLabels" . | nindent 8 }}
        component: "spark"
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "gen3kubernetes.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      volumes:
        - name: spark-userspace
          persistentVolumeClaim:
            claimName: pvc-spark-{{ .Values.ENV }}

      containers:
        - name: "spark-{{ .Values.ENV }}"
          env:
            {{- range .Values.spark.environment }}
            - name:  {{ .name  | quote }}
              value: {{ .value | quote }}
            {{- end }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.spark.image }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command: ["bash" , "-c", "python run_config.py && hdfs --daemon start namenode && hdfs --daemon start datanode && yarn --daemon start resourcemanager && yarn --daemon start nodemanager && hdfs dfsadmin -safemode leave &&  hdfs dfs -mkdir /result && while true; do sleep 5; done"]
          # command:
          #   - "bash"
          #   - "-c"
          #   - "python run_config.py && test ! -f /hadoop/hdfs/data/dfs/namenode/current/VERSION && hdfs namenode -format -force || echo existing data found, no hdfs-format needed;"
          #   - "bash"
          #   - "-c"
          #   - "python run_config.py && hdfs --daemon start namenode && hdfs --daemon start datanode && yarn --daemon start resourcemanager && yarn --daemon start nodemanager && hdfs dfsadmin -safemode leave &&  hdfs dfs -mkdir /result && while true; do sleep 5; done ;"]
          #command: ["sleep" , "60000000"]
          volumeMounts:
            - name: spark-userspace
              mountPath: /hadoop/hdfs/data
              readOnly: False
          ports:
            {{- range .Values.spark.serviceports }}
            - name: {{ .name }}
              containerPort: {{ .port }}
              protocol: "TCP"
            {{- end }}
          livenessProbe:
            tcpSocket:
              port: "eightythirty"
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            tcpSocket:
              port: "eightythirtyone"
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            {{- toYaml .Values.spark.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-spark-{{ .Values.ENV }}
spec:
  storageClassName: gen3-azurefile
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: {{ .Values.spark.userDirSize }}
